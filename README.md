# Запуск

Запустить ноутбук. Желательно с Python 3.13.

# Задача

Выбранная среда: Pendulum-v1.

Алгоритмы: PPO, A2C.

# Ход работы

## Эксперимент 1

Гипотеза: PPO будет обучаться стабильнее и получать бóльшую среднюю награду, чем A2C.

Оценка награды:

PPO: -203.91 ± 118.41

A2C: -832.82 ± 544.29

<img width="888" height="548" alt="image" src="https://github.com/user-attachments/assets/d6a2a0a6-5539-4772-a610-cd71d8599e34" />

Гипотеза оказалась верна: PPO обучается стабильнее и получает бóльшую среднюю награду, чем A2C.

## Эксперимент 2

Гипотеза: более низкий gamma (0.9) заставит агента планировать более краткосрочно, что уменьшит награду.

Средние награды:

gamma=0.9: -177.25

gamma=0.99: -155.20

<img width="897" height="549" alt="image" src="https://github.com/user-attachments/assets/3d4a6941-890c-482b-a6d0-b19ae6c61256" />

Гипотеза оказалась неверна: gamma = 0.9 гораздо быстрее сошелся и имеет практически ту же награду. Также стандартные отклонения почти на всех участках меньше/

## Визуализация агента: [видео](./ppo_pendulum-episode-0.mp4).

# Выводы

Подтвердилась первая гипотеза: PPO будет обучаться стабильнее и получать бóльшую среднюю награду, чем A2C. Скорее всего A2C делает слишком большие обновления (особенно в непрерывной среде), чего PPO избегает благодаря клиппингу ratio.

Опроверглась вторая гипотеза: "более низкий gamma (0.9) заставит агента планировать более краткосрочно, что уменьшит награду". Оба варианта сходятся к одному поведению, разница в скорости и стабильности. Поскольку награда локальная, без отсроченных бонусов, возможно минимизировать ошибку прямо сейчас выгоднее.
